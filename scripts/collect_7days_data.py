#!/usr/bin/env python3
"""
ê³¼ê±° 7ì¼ì¹˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ìˆ˜ì§‘í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

YouTube Data API v3ì˜ publishedAfter íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬
ê° ë‚ ì§œë³„ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , Bronze/Silver/Gold ì²˜ë¦¬ê¹Œì§€ ìë™ ì‹¤í–‰

API í• ë‹¹ëŸ‰ ì‚¬ìš©ëŸ‰:
- ê²€ìƒ‰ 1íšŒë‹¹ 100 units
- 7ì¼ Ã— 3ê°œ ì¿¼ë¦¬ = 21íšŒ ê²€ìƒ‰ = 2,100 units
- ì¼ì¼ í• ë‹¹ëŸ‰ 10,000 ì¤‘ 21% ì‚¬ìš©

ì‹¤í–‰:
    python scripts/collect_7days_data.py
"""

import sys
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

from video_social_rtp.core.config import load_settings, ensure_dirs
from video_social_rtp.core.logging import setup_logging


def collect_date_range_data(
    query: str,
    days_back: int = 7,
    region_code: str = "KR",
    relevance_language: str = "ko",
    max_results_per_day: int = 50
) -> List[Dict]:
    """
    ê³¼ê±° Nì¼ì¹˜ ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ ìˆ˜ì§‘

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "NewJeans")
        days_back: ê³¼ê±° ë©°ì¹ ì¹˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í• ì§€
        region_code: ì§€ì—­ ì½”ë“œ (KR, US ë“±)
        relevance_language: ì–¸ì–´ ì½”ë“œ (ko, en ë“±)
        max_results_per_day: ê° ë‚ ì§œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        ìˆ˜ì§‘ëœ ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    s = load_settings()
    log = setup_logging("collect_7days")

    api_key = s.yt_api_key
    if not api_key:
        log.error("YT_API_KEY not found in .env")
        print("âŒ YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("   .env íŒŒì¼ì— YT_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)

    try:
        from googleapiclient.discovery import build
        from googleapiclient.errors import HttpError
    except ImportError:
        log.error("google-api-python-client not installed")
        print("âŒ google-api-python-client íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        print("   pip install google-api-python-client")
        sys.exit(1)

    yt = build("youtube", "v3", developerKey=api_key)

    all_items = []
    total_api_calls = 0

    print(f"\nğŸ“… {query} ê²€ìƒ‰: ê³¼ê±° {days_back}ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)

    # ê° ë‚ ì§œë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
    for day_offset in range(days_back):
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (UTC ê¸°ì¤€)
        end_date = datetime.utcnow() - timedelta(days=day_offset)
        start_date = end_date - timedelta(days=1)

        # RFC 3339 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        published_after = start_date.strftime("%Y-%m-%dT%H:%M:%SZ")
        published_before = end_date.strftime("%Y-%m-%dT%H:%M:%SZ")

        date_str = start_date.strftime("%Y-%m-%d")

        try:
            req = yt.search().list(
                q=query,
                part="snippet",
                type="video",
                maxResults=min(max_results_per_day, 50),
                regionCode=region_code,
                relevanceLanguage=relevance_language,
                publishedAfter=published_after,
                publishedBefore=published_before,
                order="date"  # ë‚ ì§œìˆœ ì •ë ¬
            )

            res = req.execute()
            total_api_calls += 1

            day_items = []
            for i, item in enumerate(res.get("items", [])):
                id_obj = item.get("id", {}) or {}
                vid = id_obj.get("videoId")
                snippet = item.get("snippet", {}) or {}

                if not vid:
                    continue

                # ì‹¤ì œ publish ì‹œê°„ì„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜ (ì¤‘ìš”!)
                published_at = snippet.get("publishedAt", "")
                try:
                    # ISO 8601 format: 2024-01-15T12:34:56Z
                    pub_dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    ts = int(pub_dt.timestamp() * 1000)
                except Exception as e:
                    # Fallback to day start time if parsing fails
                    ts = int(start_date.timestamp() * 1000)

                day_items.append({
                    "post_id": f"search{date_str}_{i}_{vid}",
                    "text": snippet.get("title", ""),
                    "lang": relevance_language or "en",
                    "ts": ts,
                    "author_id": snippet.get("channelId", ""),
                    "video_id": vid,
                    "source": "yt",
                    "collected_date": date_str  # ìˆ˜ì§‘ ë‚ ì§œ ë©”íƒ€ë°ì´í„°
                })

            all_items.extend(day_items)

            print(f"  {date_str}: {len(day_items):3d} videos | "
                  f"Total: {len(all_items):4d} | API calls: {total_api_calls}")

            # API rate limit íšŒí”¼ (ë¶„ë‹¹ 60 quota)
            time.sleep(1)

        except HttpError as e:
            log.error(f"API error on {date_str}: {e}")
            print(f"  âš ï¸  {date_str}: API ì˜¤ë¥˜ - {e}")
            continue
        except Exception as e:
            log.error(f"Error on {date_str}: {e}")
            print(f"  âš ï¸  {date_str}: ì˜¤ë¥˜ - {e}")
            continue

    print("="*60)
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_items)} videos, {total_api_calls} API calls")
    print(f"   ì˜ˆìƒ API units ì‚¬ìš©: ~{total_api_calls * 100} (ì¼ì¼ í• ë‹¹ëŸ‰ì˜ {total_api_calls}%)")

    return all_items


def save_to_landing(items: List[Dict], query: str) -> Path:
    """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ landing ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    s = load_settings()
    ensure_dirs(s)

    landing_dir = Path(s.landing_dir)
    timestamp = int(time.time())
    filename = landing_dir / f"events_{query.replace(' ', '_')}_{timestamp}.json"

    # NDJSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
    with filename.open("w", encoding="utf-8") as f:
        for item in items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
    print(f"   íŒŒì¼ í¬ê¸°: {filename.stat().st_size / 1024:.1f} KB")

    return filename


def run_pipeline():
    """Bronze â†’ Silver â†’ Gold íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    import subprocess

    print("\n" + "="*60)
    print("ğŸ”„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
    print("="*60)

    steps = [
        ("Bronze", ["python", "-m", "video_social_rtp.cli", "bronze"]),
        ("Silver", ["python", "-m", "video_social_rtp.cli", "silver", "--once"]),
        ("Gold", ["python", "-m", "video_social_rtp.cli", "gold", "--top-pct", "0.9"])
    ]

    for step_name, cmd in steps:
        print(f"\nâ–¶ï¸  {step_name} ë‹¨ê³„ ì‹¤í–‰ ì¤‘...")
        try:
            result = subprocess.run(
                cmd,
                cwd=PROJECT_ROOT,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode == 0:
                print(f"   âœ… {step_name} ì™„ë£Œ")
            else:
                print(f"   âš ï¸  {step_name} ì‹¤íŒ¨ (code {result.returncode})")
                if result.stderr:
                    print(f"   Error: {result.stderr[:200]}")
        except Exception as e:
            print(f"   âŒ {step_name} ì˜¤ë¥˜: {e}")

    print("\n" + "="*60)
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("="*60)


def main():
    print("\n" + "="*60)
    print("ğŸ“Š K-POP 7ì¼ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸")
    print("="*60)

    # ìˆ˜ì§‘í•  ì¿¼ë¦¬ ëª©ë¡
    queries = [
        "NewJeans",
        "BTS official",
        "BLACKPINK"
    ]

    all_collected_items = []

    # ê° ì¿¼ë¦¬ë³„ë¡œ 7ì¼ì¹˜ ë°ì´í„° ìˆ˜ì§‘
    for query in queries:
        items = collect_date_range_data(
            query=query,
            days_back=7,
            region_code="KR",
            relevance_language="ko",
            max_results_per_day=50
        )

        if items:
            save_to_landing(items, query)
            all_collected_items.extend(items)

        # ì¿¼ë¦¬ ê°„ ëŒ€ê¸° (API rate limit)
        if query != queries[-1]:
            print("\nâ³ ë‹¤ìŒ ì¿¼ë¦¬ê¹Œì§€ 2ì´ˆ ëŒ€ê¸°...")
            time.sleep(2)

    # ì „ì²´ ìˆ˜ì§‘ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“ˆ ìˆ˜ì§‘ ìš”ì•½")
    print("="*60)
    print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(queries)}")
    print(f"ì´ ì˜ìƒ ìˆ˜: {len(all_collected_items)}")
    print(f"ì˜ˆìƒ API units: ~{len(queries) * 7 * 100}")

    # ë‚ ì§œë³„ ë¶„í¬ í™•ì¸
    from collections import Counter
    date_dist = Counter(item.get("collected_date", "unknown") for item in all_collected_items)
    print(f"\në‚ ì§œë³„ ë¶„í¬:")
    for date_str in sorted(date_dist.keys()):
        print(f"  {date_str}: {date_dist[date_str]:3d} videos")

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
    print("\n" + "="*60)
    response = input("\níŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Bronzeâ†’Silverâ†’Gold) [Y/n]: ")

    if response.lower() in ['y', 'yes', '']:
        run_pipeline()

        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: python examples/quick_analysis.py")
        print("  2. Streamlit UI ì‹¤í–‰: python -m video_social_rtp.cli ui --port 8501")
        print("  3. ëª¨ë¸ í•™ìŠµ: python -m video_social_rtp.cli train")
    else:
        print("\níŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        print("ìˆ˜ë™ ì‹¤í–‰: python -m video_social_rtp.cli bronze")


if __name__ == "__main__":
    main()
