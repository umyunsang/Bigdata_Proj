# 05. ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì„¤ê³„

## 1. ê°œìš”

### 1.1 ëª©í‘œ
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì œê³µ
- Streamlit ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- CDF/PDF ì‹œê°í™”ë¥¼ í†µí•œ ì˜ˆì¸¡ ê²°ê³¼ í•´ì„
- Bloom Filter ê¸°ë°˜ ì¿¼ë¦¬ ì‚¬ì „ ì ê²€

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­
- **ì‘ë‹µ ì‹œê°„**: 100ms ì´ë‚´ ì˜ˆì¸¡ ì‘ë‹µ
- **ê°€ìš©ì„±**: 99.9% ì´ìƒ ì„œë¹„ìŠ¤ ê°€ìš©ì„±
- **í™•ì¥ì„±**: ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
- **ì‚¬ìš©ì ê²½í—˜**: ì§ê´€ì ì´ê³  ë°˜ì‘í˜• UI

## 2. ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Interfaceâ”‚    â”‚  Prediction API â”‚    â”‚   Data Sources  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Streamlit UI  â”‚â—€â”€â”€â”€â”‚ â€¢ FastAPI       â”‚â—€â”€â”€â”€â”‚ â€¢ Gold Layer    â”‚
â”‚ â€¢ Dashboard     â”‚    â”‚ â€¢ Model Serving â”‚    â”‚ â€¢ Feature Store â”‚
â”‚ â€¢ Visualization â”‚    â”‚ â€¢ Caching       â”‚    â”‚ â€¢ Real-time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Monitoring    â”‚
                       â”‚                 â”‚
                       â”‚ â€¢ Performance   â”‚
                       â”‚ â€¢ Alerts        â”‚
                       â”‚ â€¢ Logs          â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ê³„ì¸µ

#### 2.2.1 API Gateway
- **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° API ì„œë²„
- **ì¸ì¦/ì¸ê°€**: JWT í† í° ê¸°ë°˜ ì¸ì¦
- **Rate Limiting**: ìš”ì²­ ì œí•œ ë° ë¶€í•˜ ë¶„ì‚°

#### 2.2.2 ì˜ˆì¸¡ ì—”ì§„
- **ëª¨ë¸ ì„œë¹™**: MLflow ê¸°ë°˜ ëª¨ë¸ ë°°í¬
- **ìºì‹±**: Redis ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ ìºì‹±
- **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ëŸ‰ ì˜ˆì¸¡ ìš”ì²­ ì²˜ë¦¬

#### 2.2.3 ë°ì´í„° ê³„ì¸µ
- **Feature Store**: ì‹¤ì‹œê°„ í”¼ì²˜ ì¡°íšŒ
- **Model Registry**: ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- **Cache Layer**: ê³ ì„±ëŠ¥ ë°ì´í„° ìºì‹±

## 3. Streamlit ëŒ€ì‹œë³´ë“œ ì„¤ê³„

### 3.1 ëŒ€ì‹œë³´ë“œ êµ¬ì¡°

#### 3.1.1 ë©”ì¸ ë ˆì´ì•„ì›ƒ
```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

def create_main_layout():
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ ë ˆì´ì•„ì›ƒ"""
    st.set_page_config(
        page_title="Video Social Analytics Dashboard",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ğŸ¯ Video Analytics")
        st.markdown("---")
        
        # í•„í„° ì˜µì…˜
        st.subheader("ğŸ“… ê¸°ê°„ ì„ íƒ")
        date_range = st.date_input(
            "ë¶„ì„ ê¸°ê°„",
            value=(datetime.now() - timedelta(days=7), datetime.now()),
            max_value=datetime.now()
        )
        
        st.subheader("ğŸ¬ ì¹´í…Œê³ ë¦¬ í•„í„°")
        categories = st.multiselect(
            "ì¹´í…Œê³ ë¦¬ ì„ íƒ",
            options=["ìŒì•…", "ê²Œì„", "ë‰´ìŠ¤", "ìŠ¤í¬ì¸ ", "êµìœ¡"],
            default=["ìŒì•…", "ê²Œì„"]
        )
        
        st.subheader("ğŸ“ˆ ì„±ëŠ¥ ì„ê³„ê°’")
        performance_threshold = st.slider(
            "ì„±ëŠ¥ ì„ê³„ê°’ (%)",
            min_value=0,
            max_value=100,
            value=90
        )
    
    # ë©”ì¸ ì»¨í…ì¸ 
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.title("ğŸ“Š ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì„±ê³¼ ë¶„ì„")
        create_performance_charts()
    
    with col2:
        st.title("ğŸ” ì˜ˆì¸¡ ê²€ìƒ‰")
        create_prediction_search()
```

#### 3.1.2 ì„±ëŠ¥ ì°¨íŠ¸
```python
def create_performance_charts():
    """ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
    # CDF/PDF ì°¨íŠ¸
    st.subheader("ğŸ“ˆ ì„±ê³¼ ë¶„í¬ (CDF/PDF)")
    
    # ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ)
    engagement_data = generate_sample_engagement_data()
    
    # CDF ì°¨íŠ¸
    fig_cdf = go.Figure()
    fig_cdf.add_trace(go.Scatter(
        x=engagement_data['engagement_score'],
        y=engagement_data['cdf'],
        mode='lines',
        name='CDF',
        line=dict(color='blue', width=2)
    ))
    fig_cdf.update_layout(
        title="Engagement Score CDF",
        xaxis_title="Engagement Score",
        yaxis_title="Cumulative Probability",
        height=400
    )
    st.plotly_chart(fig_cdf, use_container_width=True)
    
    # PDF ì°¨íŠ¸
    fig_pdf = go.Figure()
    fig_pdf.add_trace(go.Bar(
        x=engagement_data['engagement_score'],
        y=engagement_data['pdf'],
        name='PDF',
        marker_color='lightblue'
    ))
    fig_pdf.update_layout(
        title="Engagement Score PDF",
        xaxis_title="Engagement Score",
        yaxis_title="Probability Density",
        height=400
    )
    st.plotly_chart(fig_pdf, use_container_width=True)
```

### 3.2 ì˜ˆì¸¡ ê²€ìƒ‰ ê¸°ëŠ¥

#### 3.2.1 ë¹„ë””ì˜¤ ê²€ìƒ‰
```python
def create_prediction_search():
    """ì˜ˆì¸¡ ê²€ìƒ‰ ê¸°ëŠ¥"""
    st.subheader("ğŸ” ë¹„ë””ì˜¤ ì˜ˆì¸¡ ê²€ìƒ‰")
    
    # ê²€ìƒ‰ ì˜µì…˜
    search_type = st.radio(
        "ê²€ìƒ‰ ìœ í˜•",
        options=["ë¹„ë””ì˜¤ ID", "í‚¤ì›Œë“œ", "ì±„ë„"],
        horizontal=True
    )
    
    if search_type == "ë¹„ë””ì˜¤ ID":
        video_id = st.text_input("ë¹„ë””ì˜¤ ID ì…ë ¥", placeholder="dQw4w9WgXcQ")
        if st.button("ì˜ˆì¸¡ ì‹¤í–‰"):
            if video_id:
                prediction_result = get_video_prediction(video_id)
                display_prediction_result(prediction_result)
    
    elif search_type == "í‚¤ì›Œë“œ":
        keyword = st.text_input("í‚¤ì›Œë“œ ì…ë ¥", placeholder="K-pop")
        if st.button("ì˜ˆì¸¡ ì‹¤í–‰"):
            if keyword:
                prediction_results = get_keyword_predictions(keyword)
                display_keyword_results(prediction_results)
    
    elif search_type == "ì±„ë„":
        channel_id = st.text_input("ì±„ë„ ID ì…ë ¥", placeholder="UCuAXFkgsw1L7xaCfnd5JJOw")
        if st.button("ì˜ˆì¸¡ ì‹¤í–‰"):
            if channel_id:
                channel_predictions = get_channel_predictions(channel_id)
                display_channel_results(channel_predictions)
```

#### 3.2.2 ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
```python
def display_prediction_result(result):
    """ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ"""
    if result is None:
        st.error("âŒ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ ì •ë³´
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ğŸ¯ ì˜ˆì¸¡ ì„±ê³¼", f"{result['performance_score']:.1f}%")
    
    with col2:
        st.metric("ğŸ“Š ì‹ ë¢°ë„", f"{result['confidence']:.1f}%")
    
    with col3:
        st.metric("ğŸ† ìˆœìœ„", f"ìƒìœ„ {result['percentile']:.1f}%")
    
    # ìƒì„¸ ì •ë³´
    st.subheader("ğŸ“‹ ìƒì„¸ ë¶„ì„")
    
    # ì„±ê³¼ ì§€í‘œ
    metrics_col1, metrics_col2 = st.columns(2)
    
    with metrics_col1:
        st.write("**ğŸ“ˆ ì„±ê³¼ ì§€í‘œ**")
        st.write(f"- ì¡°íšŒìˆ˜: {result['view_count']:,}")
        st.write(f"- ì¢‹ì•„ìš”: {result['like_count']:,}")
        st.write(f"- ëŒ“ê¸€: {result['comment_count']:,}")
        st.write(f"- ì°¸ì—¬ë„: {result['engagement_rate']:.2f}")
    
    with metrics_col2:
        st.write("**ğŸ”® ì˜ˆì¸¡ ê²°ê³¼**")
        st.write(f"- íŠ¸ë Œë“œ ì˜ˆì¸¡: {result['trend_prediction']}")
        st.write(f"- ë°”ì´ëŸ´ í™•ë¥ : {result['viral_probability']:.1f}%")
        st.write(f"- ì„±ì¥ ì ì¬ë ¥: {result['growth_potential']}")
    
    # Bloom Filter ìƒíƒœ
    if result.get('bloom_filter_status'):
        st.success("âœ… ìµœê·¼ 7ì¼ ë°ì´í„°ì— ì¡´ì¬")
    else:
        st.warning("âš ï¸ ìµœê·¼ 7ì¼ ë°ì´í„°ì— ì—†ìŒ (ì˜¤ë˜ëœ ë°ì´í„°)")
```

### 3.3 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

#### 3.3.1 ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
```python
def create_system_monitoring():
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
    st.subheader("âš¡ ì‹œìŠ¤í…œ ìƒíƒœ")
    
    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ”„ ì²˜ë¦¬ëŸ‰", "1,234 req/min", "12%")
    
    with col2:
        st.metric("â±ï¸ ì‘ë‹µì‹œê°„", "45ms", "-5ms")
    
    with col3:
        st.metric("ğŸ’¾ ë©”ëª¨ë¦¬", "67%", "3%")
    
    with col4:
        st.metric("ğŸ–¥ï¸ CPU", "34%", "-2%")
    
    # ì‹¤ì‹œê°„ ì°¨íŠ¸
    st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥")
    
    # ìƒ˜í”Œ ì‹¤ì‹œê°„ ë°ì´í„°
    performance_data = generate_realtime_performance_data()
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=performance_data['timestamp'],
        y=performance_data['response_time'],
        mode='lines',
        name='ì‘ë‹µì‹œê°„ (ms)',
        line=dict(color='blue')
    ))
    fig.add_trace(go.Scatter(
        x=performance_data['timestamp'],
        y=performance_data['throughput'],
        mode='lines',
        name='ì²˜ë¦¬ëŸ‰ (req/min)',
        yaxis='y2',
        line=dict(color='red')
    ))
    
    fig.update_layout(
        title="ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ",
        xaxis_title="ì‹œê°„",
        yaxis_title="ì‘ë‹µì‹œê°„ (ms)",
        yaxis2=dict(title="ì²˜ë¦¬ëŸ‰ (req/min)", overlaying="y", side="right"),
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
```

## 4. FastAPI ì˜ˆì¸¡ ì„œë¹„ìŠ¤

### 4.1 API ì—”ë“œí¬ì¸íŠ¸ ì„¤ê³„

#### 4.1.1 ê¸°ë³¸ API êµ¬ì¡°
```python
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import asyncio
import redis
import joblib

app = FastAPI(
    title="Video Social Analytics API",
    description="ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì„±ê³¼ ì˜ˆì¸¡ API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Redis ì—°ê²°
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class VideoPredictionRequest(BaseModel):
    video_id: str
    features: Optional[dict] = None

class VideoPredictionResponse(BaseModel):
    video_id: str
    performance_score: float
    confidence: float
    percentile: float
    trend_prediction: str
    viral_probability: float
    bloom_filter_status: bool
    processing_time: float

class KeywordSearchRequest(BaseModel):
    keyword: str
    limit: int = 10

class KeywordSearchResponse(BaseModel):
    keyword: str
    results: List[dict]
    total_count: int
    processing_time: float
```

#### 4.1.2 ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
```python
@app.post("/predict/video", response_model=VideoPredictionResponse)
async def predict_video_performance(request: VideoPredictionRequest):
    """ë¹„ë””ì˜¤ ì„±ê³¼ ì˜ˆì¸¡"""
    start_time = time.time()
    
    try:
        # ìºì‹œ í™•ì¸
        cache_key = f"prediction:{request.video_id}"
        cached_result = redis_client.get(cache_key)
        
        if cached_result:
            result = json.loads(cached_result)
            result['processing_time'] = time.time() - start_time
            return VideoPredictionResponse(**result)
        
        # ë¹„ë””ì˜¤ ë°ì´í„° ì¡°íšŒ
        video_data = await get_video_data(request.video_id)
        if not video_data:
            raise HTTPException(status_code=404, detail="Video not found")
        
        # í”¼ì²˜ ì¶”ì¶œ
        features = await extract_features(video_data)
        
        # ëª¨ë¸ ì˜ˆì¸¡
        prediction = await predict_with_model(features)
        
        # Bloom Filter ìƒíƒœ í™•ì¸
        bloom_status = await check_bloom_filter_status(request.video_id)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "video_id": request.video_id,
            "performance_score": prediction['performance_score'],
            "confidence": prediction['confidence'],
            "percentile": prediction['percentile'],
            "trend_prediction": prediction['trend_prediction'],
            "viral_probability": prediction['viral_probability'],
            "bloom_filter_status": bloom_status,
            "processing_time": time.time() - start_time
        }
        
        # ìºì‹œ ì €ì¥ (5ë¶„ TTL)
        redis_client.setex(cache_key, 300, json.dumps(result))
        
        return VideoPredictionResponse(**result)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/keyword", response_model=KeywordSearchResponse)
async def search_by_keyword(request: KeywordSearchRequest):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
    start_time = time.time()
    
    try:
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        search_results = await search_videos_by_keyword(
            request.keyword, 
            limit=request.limit
        )
        
        # ê° ê²°ê³¼ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = []
        for video in search_results:
            prediction = await predict_video_performance(
                VideoPredictionRequest(video_id=video['video_id'])
            )
            predictions.append({
                "video_id": video['video_id'],
                "title": video['title'],
                "prediction": prediction.dict()
            })
        
        return KeywordSearchResponse(
            keyword=request.keyword,
            results=predictions,
            total_count=len(predictions),
            processing_time=time.time() - start_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 4.2 ëª¨ë¸ ì„œë¹™

#### 4.2.1 ëª¨ë¸ ë¡œë”
```python
class ModelLoader:
    def __init__(self):
        self.models = {}
        self.model_metadata = {}
    
    async def load_model(self, model_name: str, version: str = "latest"):
        """ëª¨ë¸ ë¡œë“œ"""
        model_key = f"{model_name}_{version}"
        
        if model_key not in self.models:
            # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            model_path = f"models/{model_name}/{version}/model.pkl"
            metadata_path = f"models/{model_name}/{version}/metadata.json"
            
            # ëª¨ë¸ ë¡œë“œ
            self.models[model_key] = joblib.load(model_path)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_path, 'r') as f:
                self.model_metadata[model_key] = json.load(f)
        
        return self.models[model_key], self.model_metadata[model_key]
    
    async def predict(self, model_name: str, features: dict, version: str = "latest"):
        """ëª¨ë¸ ì˜ˆì¸¡"""
        model, metadata = await self.load_model(model_name, version)
        
        # í”¼ì²˜ ë²¡í„° ìƒì„±
        feature_vector = self._create_feature_vector(features, metadata['feature_list'])
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        prediction = model.predict([feature_vector])[0]
        probability = model.predict_proba([feature_vector])[0]
        
        return {
            "prediction": prediction,
            "probability": probability,
            "confidence": max(probability)
        }
    
    def _create_feature_vector(self, features: dict, feature_list: List[str]):
        """í”¼ì²˜ ë²¡í„° ìƒì„±"""
        vector = []
        for feature_name in feature_list:
            vector.append(features.get(feature_name, 0.0))
        return vector

# ì „ì—­ ëª¨ë¸ ë¡œë”
model_loader = ModelLoader()
```

#### 4.2.2 í”¼ì²˜ ì¶”ì¶œ
```python
async def extract_features(video_data: dict) -> dict:
    """ë¹„ë””ì˜¤ ë°ì´í„°ì—ì„œ í”¼ì²˜ ì¶”ì¶œ"""
    features = {}
    
    # ê¸°ë³¸ í”¼ì²˜
    features['view_count'] = video_data.get('view_count', 0)
    features['like_count'] = video_data.get('like_count', 0)
    features['comment_count'] = video_data.get('comment_count', 0)
    features['duration'] = video_data.get('duration', 0)
    
    # íŒŒìƒ í”¼ì²˜
    if features['view_count'] > 0:
        features['like_view_ratio'] = features['like_count'] / features['view_count']
        features['comment_view_ratio'] = features['comment_count'] / features['view_count']
        features['engagement_rate'] = (features['like_count'] + features['comment_count']) / features['view_count']
    else:
        features['like_view_ratio'] = 0
        features['comment_view_ratio'] = 0
        features['engagement_rate'] = 0
    
    # ì‹œê°„ í”¼ì²˜
    published_at = video_data.get('published_at')
    if published_at:
        dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
        features['hour'] = dt.hour
        features['day_of_week'] = dt.weekday()
        features['is_weekend'] = 1 if dt.weekday() >= 5 else 0
    
    # ì¹´í…Œê³ ë¦¬ í”¼ì²˜
    features['category_id'] = video_data.get('category_id', 0)
    
    return features
```

### 4.3 ìºì‹± ì „ëµ

#### 4.3.1 ë‹¤ì¸µ ìºì‹±
```python
class MultiLayerCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.memory_cache = {}
        self.cache_ttl = {
            'prediction': 300,  # 5ë¶„
            'features': 600,    # 10ë¶„
            'search': 180       # 3ë¶„
        }
    
    async def get(self, key: str, cache_type: str = 'prediction'):
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # Redis ìºì‹œ í™•ì¸
        cached_data = self.redis_client.get(key)
        if cached_data:
            data = json.loads(cached_data)
            # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
            self.memory_cache[key] = data
            return data
        
        return None
    
    async def set(self, key: str, data: dict, cache_type: str = 'prediction'):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        ttl = self.cache_ttl.get(cache_type, 300)
        
        # Redisì— ì €ì¥
        self.redis_client.setex(key, ttl, json.dumps(data))
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        self.memory_cache[key] = data
    
    async def invalidate(self, pattern: str):
        """íŒ¨í„´ì— ë§ëŠ” ìºì‹œ ë¬´íš¨í™”"""
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)
        
        # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œë„ ì œê±°
        for key in list(self.memory_cache.keys()):
            if pattern.replace('*', '') in key:
                del self.memory_cache[key]
```

## 5. Bloom Filter í†µí•©

### 5.1 Bloom Filter ì„œë¹„ìŠ¤

#### 5.1.1 Bloom Filter API
```python
class BloomFilterService:
    def __init__(self):
        self.bloom_filters = {}
        self.redis_client = redis.Redis(host='localhost', port=6379)
    
    async def check_video_exists(self, video_id: str, time_window_days: int = 7) -> bool:
        """ë¹„ë””ì˜¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        # ì‹œê°„ ìœˆë„ìš°ë³„ Bloom Filter ë¡œë“œ
        current_date = datetime.now().date()
        cutoff_date = current_date - timedelta(days=time_window_days)
        
        for i in range(time_window_days):
            check_date = cutoff_date + timedelta(days=i)
            filter_key = f"bloom_filter:{check_date}"
            
            # Redisì—ì„œ Bloom Filter ë¡œë“œ
            bloom_data = self.redis_client.get(filter_key)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                if bloom_filter.might_contain(video_id):
                    return True
        
        return False
    
    async def add_video_to_bloom(self, video_id: str, date: datetime = None):
        """Bloom Filterì— ë¹„ë””ì˜¤ ì¶”ê°€"""
        if date is None:
            date = datetime.now()
        
        date_key = date.date()
        filter_key = f"bloom_filter:{date_key}"
        
        # ê¸°ì¡´ Bloom Filter ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        bloom_data = self.redis_client.get(filter_key)
        if bloom_data:
            bloom_filter = pickle.loads(bloom_data)
        else:
            bloom_filter = DynamicBloomFilter(initial_capacity=100000, error_rate=0.01)
        
        # ë¹„ë””ì˜¤ ID ì¶”ê°€
        bloom_filter.add(video_id)
        
        # ì—…ë°ì´íŠ¸ëœ Bloom Filter ì €ì¥
        self.redis_client.setex(filter_key, 86400, pickle.dumps(bloom_filter))  # 24ì‹œê°„ TTL
    
    async def get_bloom_filter_stats(self) -> dict:
        """Bloom Filter í†µê³„"""
        stats = {
            "total_filters": 0,
            "total_capacity": 0,
            "estimated_items": 0,
            "false_positive_rate": 0.0
        }
        
        # ìµœê·¼ 7ì¼ í•„í„° í†µê³„
        current_date = datetime.now().date()
        for i in range(7):
            check_date = current_date - timedelta(days=i)
            filter_key = f"bloom_filter:{check_date}"
            
            bloom_data = self.redis_client.get(filter_key)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                stats["total_filters"] += 1
                stats["total_capacity"] += bloom_filter.m
                stats["estimated_items"] += bloom_filter.count()
                stats["false_positive_rate"] += bloom_filter.error_rate
        
        if stats["total_filters"] > 0:
            stats["false_positive_rate"] /= stats["total_filters"]
        
        return stats
```

### 5.2 ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

#### 5.2.1 ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸
```python
async def update_bloom_filter_stream():
    """ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¡œ Bloom Filter ì—…ë°ì´íŠ¸"""
    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
    streaming_df = spark.readStream \
        .format("delta") \
        .option("path", "data/silver/videos") \
        .load()
    
    def process_batch(batch_df, batch_id):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        bloom_service = BloomFilterService()
        
        for row in batch_df.collect():
            video_id = row['video_id']
            timestamp = row['event_timestamp']
            
            # Bloom Filterì— ì¶”ê°€
            asyncio.run(bloom_service.add_video_to_bloom(video_id, timestamp))
    
    # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘
    query = streaming_df.writeStream \
        .foreachBatch(process_batch) \
        .option("checkpointLocation", "checkpoints/bloom_filter") \
        .start()
    
    return query
```

## 6. ì„±ëŠ¥ ìµœì í™”

### 6.1 ë¹„ë™ê¸° ì²˜ë¦¬

#### 6.1.1 ë¹„ë™ê¸° ì˜ˆì¸¡
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncPredictionService:
    def __init__(self, max_workers: int = 10):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.model_loader = ModelLoader()
        self.cache = MultiLayerCache()
    
    async def predict_batch(self, video_ids: List[str]) -> List[dict]:
        """ë°°ì¹˜ ì˜ˆì¸¡"""
        # ë¹„ë™ê¸° ì‘ì—… ìƒì„±
        tasks = [self._predict_single(video_id) for video_id in video_ids]
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "video_id": video_ids[i],
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _predict_single(self, video_id: str) -> dict:
        """ë‹¨ì¼ ë¹„ë””ì˜¤ ì˜ˆì¸¡"""
        # ìºì‹œ í™•ì¸
        cache_key = f"prediction:{video_id}"
        cached_result = await self.cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # ë°ì´í„° ì¡°íšŒ ë° ì˜ˆì¸¡
        video_data = await get_video_data(video_id)
        features = await extract_features(video_data)
        prediction = await self.model_loader.predict("video_performance", features)
        
        # ê²°ê³¼ ìºì‹±
        result = {
            "video_id": video_id,
            "prediction": prediction,
            "timestamp": datetime.now().isoformat()
        }
        
        await self.cache.set(cache_key, result)
        
        return result
```

### 6.2 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

#### 6.2.1 ì—°ê²° í’€ë§
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseManager:
    def __init__(self, connection_string: str):
        self.engine = create_engine(
            connection_string,
            poolclass=QueuePool,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )
    
    async def get_video_data(self, video_id: str) -> dict:
        """ë¹„ë””ì˜¤ ë°ì´í„° ì¡°íšŒ"""
        query = """
        SELECT video_id, title, view_count, like_count, comment_count, 
               published_at, category_id, duration
        FROM videos 
        WHERE video_id = %s
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (video_id,))
            row = result.fetchone()
            
            if row:
                return dict(row._mapping)
            return None
    
    async def search_videos_by_keyword(self, keyword: str, limit: int = 10) -> List[dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ê²€ìƒ‰"""
        query = """
        SELECT video_id, title, view_count, like_count, comment_count,
               published_at, category_id
        FROM videos 
        WHERE title ILIKE %s OR description ILIKE %s
        ORDER BY view_count DESC
        LIMIT %s
        """
        
        search_term = f"%{keyword}%"
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (search_term, search_term, limit))
            rows = result.fetchall()
            
            return [dict(row._mapping) for row in rows]
```

## 7. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 7.1 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### 7.1.1 ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
import time
from prometheus_client import Counter, Histogram, Gauge, start_http_server

class MetricsCollector:
    def __init__(self):
        # ìš”ì²­ ë©”íŠ¸ë¦­
        self.request_count = Counter('api_requests_total', 'Total API requests', ['endpoint', 'method'])
        self.request_duration = Histogram('api_request_duration_seconds', 'API request duration', ['endpoint'])
        
        # ì˜ˆì¸¡ ë©”íŠ¸ë¦­
        self.prediction_count = Counter('predictions_total', 'Total predictions', ['model_name'])
        self.prediction_duration = Histogram('prediction_duration_seconds', 'Prediction duration', ['model_name'])
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
        self.active_connections = Gauge('active_connections', 'Active connections')
        self.cache_hit_ratio = Gauge('cache_hit_ratio', 'Cache hit ratio')
        
        # Prometheus ì„œë²„ ì‹œì‘
        start_http_server(8000)
    
    def record_request(self, endpoint: str, method: str, duration: float):
        """ìš”ì²­ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.request_count.labels(endpoint=endpoint, method=method).inc()
        self.request_duration.labels(endpoint=endpoint).observe(duration)
    
    def record_prediction(self, model_name: str, duration: float):
        """ì˜ˆì¸¡ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.prediction_count.labels(model_name=model_name).inc()
        self.prediction_duration.labels(model_name=model_name).observe(duration)
    
    def update_system_metrics(self, active_connections: int, cache_hit_ratio: float):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.active_connections.set(active_connections)
        self.cache_hit_ratio.set(cache_hit_ratio)

# ì „ì—­ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
metrics_collector = MetricsCollector()
```

#### 7.1.2 ì•Œë¦¼ ì‹œìŠ¤í…œ
```python
class AlertManager:
    def __init__(self):
        self.alert_rules = {
            "high_response_time": {"threshold": 1.0, "severity": "warning"},
            "low_cache_hit_ratio": {"threshold": 0.7, "severity": "critical"},
            "high_error_rate": {"threshold": 0.05, "severity": "critical"}
        }
        self.alert_history = []
    
    async def check_alerts(self, metrics: dict):
        """ì•Œë¦¼ ê·œì¹™ í™•ì¸"""
        alerts = []
        
        # ì‘ë‹µ ì‹œê°„ ì•Œë¦¼
        if metrics.get('avg_response_time', 0) > self.alert_rules["high_response_time"]["threshold"]:
            alerts.append({
                "type": "high_response_time",
                "message": f"Average response time is {metrics['avg_response_time']:.2f}s",
                "severity": "warning",
                "timestamp": datetime.now()
            })
        
        # ìºì‹œ íˆíŠ¸ìœ¨ ì•Œë¦¼
        if metrics.get('cache_hit_ratio', 1.0) < self.alert_rules["low_cache_hit_ratio"]["threshold"]:
            alerts.append({
                "type": "low_cache_hit_ratio",
                "message": f"Cache hit ratio is {metrics['cache_hit_ratio']:.2f}",
                "severity": "critical",
                "timestamp": datetime.now()
            })
        
        # ì—ëŸ¬ìœ¨ ì•Œë¦¼
        if metrics.get('error_rate', 0) > self.alert_rules["high_error_rate"]["threshold"]:
            alerts.append({
                "type": "high_error_rate",
                "message": f"Error rate is {metrics['error_rate']:.2f}",
                "severity": "critical",
                "timestamp": datetime.now()
            })
        
        # ì•Œë¦¼ ì „ì†¡
        for alert in alerts:
            await self.send_alert(alert)
            self.alert_history.append(alert)
    
    async def send_alert(self, alert: dict):
        """ì•Œë¦¼ ì „ì†¡"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Slack, ì´ë©”ì¼, SMS ë“±ìœ¼ë¡œ ì „ì†¡
        print(f"ALERT: {alert['severity'].upper()} - {alert['message']}")
```

---

**ì´ì „ ë¬¸ì„œ**: [04_ëª¨ë¸_í•™ìŠµ_ë°_ìµœì í™”_ì„¤ê³„.md](./04_ëª¨ë¸_í•™ìŠµ_ë°_ìµœì í™”_ì„¤ê³„.md)  
**ë¬¸ì„œ ì™„ë£Œ**: ëª¨ë“  ì„¤ê³„ ë¬¸ì„œê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

