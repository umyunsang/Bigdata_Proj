# 05. 실시간 예측 시스템 설계

## 1. 개요

### 1.1 목표
- 실시간 예측 서비스 제공
- Streamlit 기반 대시보드 구축
- CDF/PDF 시각화를 통한 예측 결과 해석
- Bloom Filter 기반 쿼리 사전 점검

### 1.2 핵심 요구사항
- **응답 시간**: 100ms 이내 예측 응답
- **가용성**: 99.9% 이상 서비스 가용성
- **확장성**: 수평 확장 가능한 아키텍처
- **사용자 경험**: 직관적이고 반응형 UI

## 2. 실시간 예측 아키텍처

### 2.1 전체 시스템 구조

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   User Interface│    │  Prediction API │    │   Data Sources  │
│                 │    │                 │    │                 │
│ • Streamlit UI  │◀───│ • FastAPI       │◀───│ • Gold Layer    │
│ • Dashboard     │    │ • Model Serving │    │ • Feature Store │
│ • Visualization │    │ • Caching       │    │ • Real-time     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌─────────────────┐
                       │   Monitoring    │
                       │                 │
                       │ • Performance   │
                       │ • Alerts        │
                       │ • Logs          │
                       └─────────────────┘
```

### 2.2 예측 서비스 계층

#### 2.2.1 API Gateway
- **FastAPI**: 고성능 비동기 API 서버
- **인증/인가**: JWT 토큰 기반 인증
- **Rate Limiting**: 요청 제한 및 부하 분산

#### 2.2.2 예측 엔진
- **모델 서빙**: MLflow 기반 모델 배포
- **캐싱**: Redis 기반 예측 결과 캐싱
- **배치 처리**: 대량 예측 요청 처리

#### 2.2.3 데이터 계층
- **Feature Store**: 실시간 피처 조회
- **Model Registry**: 모델 버전 관리
- **Cache Layer**: 고성능 데이터 캐싱

## 3. Streamlit 대시보드 설계

### 3.1 대시보드 구조

#### 3.1.1 메인 레이아웃
```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

def create_main_layout():
    """메인 대시보드 레이아웃"""
    st.set_page_config(
        page_title="Video Social Analytics Dashboard",
        page_icon="📊",
        layout="wide"
    )
    
    # 사이드바
    with st.sidebar:
        st.title("🎯 Video Analytics")
        st.markdown("---")
        
        # 필터 옵션
        st.subheader("📅 기간 선택")
        date_range = st.date_input(
            "분석 기간",
            value=(datetime.now() - timedelta(days=7), datetime.now()),
            max_value=datetime.now()
        )
        
        st.subheader("🎬 카테고리 필터")
        categories = st.multiselect(
            "카테고리 선택",
            options=["음악", "게임", "뉴스", "스포츠", "교육"],
            default=["음악", "게임"]
        )
        
        st.subheader("📈 성능 임계값")
        performance_threshold = st.slider(
            "성능 임계값 (%)",
            min_value=0,
            max_value=100,
            value=90
        )
    
    # 메인 컨텐츠
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.title("📊 실시간 비디오 성과 분석")
        create_performance_charts()
    
    with col2:
        st.title("🔍 예측 검색")
        create_prediction_search()
```

#### 3.1.2 성능 차트
```python
def create_performance_charts():
    """성능 차트 생성"""
    # CDF/PDF 차트
    st.subheader("📈 성과 분포 (CDF/PDF)")
    
    # 샘플 데이터 (실제로는 데이터베이스에서 조회)
    engagement_data = generate_sample_engagement_data()
    
    # CDF 차트
    fig_cdf = go.Figure()
    fig_cdf.add_trace(go.Scatter(
        x=engagement_data['engagement_score'],
        y=engagement_data['cdf'],
        mode='lines',
        name='CDF',
        line=dict(color='blue', width=2)
    ))
    fig_cdf.update_layout(
        title="Engagement Score CDF",
        xaxis_title="Engagement Score",
        yaxis_title="Cumulative Probability",
        height=400
    )
    st.plotly_chart(fig_cdf, use_container_width=True)
    
    # PDF 차트
    fig_pdf = go.Figure()
    fig_pdf.add_trace(go.Bar(
        x=engagement_data['engagement_score'],
        y=engagement_data['pdf'],
        name='PDF',
        marker_color='lightblue'
    ))
    fig_pdf.update_layout(
        title="Engagement Score PDF",
        xaxis_title="Engagement Score",
        yaxis_title="Probability Density",
        height=400
    )
    st.plotly_chart(fig_pdf, use_container_width=True)
```

### 3.2 예측 검색 기능

#### 3.2.1 비디오 검색
```python
def create_prediction_search():
    """예측 검색 기능"""
    st.subheader("🔍 비디오 예측 검색")
    
    # 검색 옵션
    search_type = st.radio(
        "검색 유형",
        options=["비디오 ID", "키워드", "채널"],
        horizontal=True
    )
    
    if search_type == "비디오 ID":
        video_id = st.text_input("비디오 ID 입력", placeholder="dQw4w9WgXcQ")
        if st.button("예측 실행"):
            if video_id:
                prediction_result = get_video_prediction(video_id)
                display_prediction_result(prediction_result)
    
    elif search_type == "키워드":
        keyword = st.text_input("키워드 입력", placeholder="K-pop")
        if st.button("예측 실행"):
            if keyword:
                prediction_results = get_keyword_predictions(keyword)
                display_keyword_results(prediction_results)
    
    elif search_type == "채널":
        channel_id = st.text_input("채널 ID 입력", placeholder="UCuAXFkgsw1L7xaCfnd5JJOw")
        if st.button("예측 실행"):
            if channel_id:
                channel_predictions = get_channel_predictions(channel_id)
                display_channel_results(channel_predictions)
```

#### 3.2.2 예측 결과 표시
```python
def display_prediction_result(result):
    """예측 결과 표시"""
    if result is None:
        st.error("❌ 비디오를 찾을 수 없습니다.")
        return
    
    # 기본 정보
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("🎯 예측 성과", f"{result['performance_score']:.1f}%")
    
    with col2:
        st.metric("📊 신뢰도", f"{result['confidence']:.1f}%")
    
    with col3:
        st.metric("🏆 순위", f"상위 {result['percentile']:.1f}%")
    
    # 상세 정보
    st.subheader("📋 상세 분석")
    
    # 성과 지표
    metrics_col1, metrics_col2 = st.columns(2)
    
    with metrics_col1:
        st.write("**📈 성과 지표**")
        st.write(f"- 조회수: {result['view_count']:,}")
        st.write(f"- 좋아요: {result['like_count']:,}")
        st.write(f"- 댓글: {result['comment_count']:,}")
        st.write(f"- 참여도: {result['engagement_rate']:.2f}")
    
    with metrics_col2:
        st.write("**🔮 예측 결과**")
        st.write(f"- 트렌드 예측: {result['trend_prediction']}")
        st.write(f"- 바이럴 확률: {result['viral_probability']:.1f}%")
        st.write(f"- 성장 잠재력: {result['growth_potential']}")
    
    # Bloom Filter 상태
    if result.get('bloom_filter_status'):
        st.success("✅ 최근 7일 데이터에 존재")
    else:
        st.warning("⚠️ 최근 7일 데이터에 없음 (오래된 데이터)")
```

### 3.3 실시간 모니터링

#### 3.3.1 시스템 상태 모니터링
```python
def create_system_monitoring():
    """시스템 모니터링 대시보드"""
    st.subheader("⚡ 시스템 상태")
    
    # 시스템 메트릭
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("🔄 처리량", "1,234 req/min", "12%")
    
    with col2:
        st.metric("⏱️ 응답시간", "45ms", "-5ms")
    
    with col3:
        st.metric("💾 메모리", "67%", "3%")
    
    with col4:
        st.metric("🖥️ CPU", "34%", "-2%")
    
    # 실시간 차트
    st.subheader("📊 실시간 성능")
    
    # 샘플 실시간 데이터
    performance_data = generate_realtime_performance_data()
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=performance_data['timestamp'],
        y=performance_data['response_time'],
        mode='lines',
        name='응답시간 (ms)',
        line=dict(color='blue')
    ))
    fig.add_trace(go.Scatter(
        x=performance_data['timestamp'],
        y=performance_data['throughput'],
        mode='lines',
        name='처리량 (req/min)',
        yaxis='y2',
        line=dict(color='red')
    ))
    
    fig.update_layout(
        title="실시간 성능 지표",
        xaxis_title="시간",
        yaxis_title="응답시간 (ms)",
        yaxis2=dict(title="처리량 (req/min)", overlaying="y", side="right"),
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
```

## 4. FastAPI 예측 서비스

### 4.1 API 엔드포인트 설계

#### 4.1.1 기본 API 구조
```python
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import asyncio
import redis
import joblib

app = FastAPI(
    title="Video Social Analytics API",
    description="실시간 비디오 성과 예측 API",
    version="1.0.0"
)

# CORS 설정
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Redis 연결
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# 요청/응답 모델
class VideoPredictionRequest(BaseModel):
    video_id: str
    features: Optional[dict] = None

class VideoPredictionResponse(BaseModel):
    video_id: str
    performance_score: float
    confidence: float
    percentile: float
    trend_prediction: str
    viral_probability: float
    bloom_filter_status: bool
    processing_time: float

class KeywordSearchRequest(BaseModel):
    keyword: str
    limit: int = 10

class KeywordSearchResponse(BaseModel):
    keyword: str
    results: List[dict]
    total_count: int
    processing_time: float
```

#### 4.1.2 예측 엔드포인트
```python
@app.post("/predict/video", response_model=VideoPredictionResponse)
async def predict_video_performance(request: VideoPredictionRequest):
    """비디오 성과 예측"""
    start_time = time.time()
    
    try:
        # 캐시 확인
        cache_key = f"prediction:{request.video_id}"
        cached_result = redis_client.get(cache_key)
        
        if cached_result:
            result = json.loads(cached_result)
            result['processing_time'] = time.time() - start_time
            return VideoPredictionResponse(**result)
        
        # 비디오 데이터 조회
        video_data = await get_video_data(request.video_id)
        if not video_data:
            raise HTTPException(status_code=404, detail="Video not found")
        
        # 피처 추출
        features = await extract_features(video_data)
        
        # 모델 예측
        prediction = await predict_with_model(features)
        
        # Bloom Filter 상태 확인
        bloom_status = await check_bloom_filter_status(request.video_id)
        
        # 결과 구성
        result = {
            "video_id": request.video_id,
            "performance_score": prediction['performance_score'],
            "confidence": prediction['confidence'],
            "percentile": prediction['percentile'],
            "trend_prediction": prediction['trend_prediction'],
            "viral_probability": prediction['viral_probability'],
            "bloom_filter_status": bloom_status,
            "processing_time": time.time() - start_time
        }
        
        # 캐시 저장 (5분 TTL)
        redis_client.setex(cache_key, 300, json.dumps(result))
        
        return VideoPredictionResponse(**result)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search/keyword", response_model=KeywordSearchResponse)
async def search_by_keyword(request: KeywordSearchRequest):
    """키워드 기반 검색"""
    start_time = time.time()
    
    try:
        # 키워드 검색
        search_results = await search_videos_by_keyword(
            request.keyword, 
            limit=request.limit
        )
        
        # 각 결과에 대해 예측 수행
        predictions = []
        for video in search_results:
            prediction = await predict_video_performance(
                VideoPredictionRequest(video_id=video['video_id'])
            )
            predictions.append({
                "video_id": video['video_id'],
                "title": video['title'],
                "prediction": prediction.dict()
            })
        
        return KeywordSearchResponse(
            keyword=request.keyword,
            results=predictions,
            total_count=len(predictions),
            processing_time=time.time() - start_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 4.2 모델 서빙

#### 4.2.1 모델 로더
```python
class ModelLoader:
    def __init__(self):
        self.models = {}
        self.model_metadata = {}
    
    async def load_model(self, model_name: str, version: str = "latest"):
        """모델 로드"""
        model_key = f"{model_name}_{version}"
        
        if model_key not in self.models:
            # 모델 파일 경로
            model_path = f"models/{model_name}/{version}/model.pkl"
            metadata_path = f"models/{model_name}/{version}/metadata.json"
            
            # 모델 로드
            self.models[model_key] = joblib.load(model_path)
            
            # 메타데이터 로드
            with open(metadata_path, 'r') as f:
                self.model_metadata[model_key] = json.load(f)
        
        return self.models[model_key], self.model_metadata[model_key]
    
    async def predict(self, model_name: str, features: dict, version: str = "latest"):
        """모델 예측"""
        model, metadata = await self.load_model(model_name, version)
        
        # 피처 벡터 생성
        feature_vector = self._create_feature_vector(features, metadata['feature_list'])
        
        # 예측 수행
        prediction = model.predict([feature_vector])[0]
        probability = model.predict_proba([feature_vector])[0]
        
        return {
            "prediction": prediction,
            "probability": probability,
            "confidence": max(probability)
        }
    
    def _create_feature_vector(self, features: dict, feature_list: List[str]):
        """피처 벡터 생성"""
        vector = []
        for feature_name in feature_list:
            vector.append(features.get(feature_name, 0.0))
        return vector

# 전역 모델 로더
model_loader = ModelLoader()
```

#### 4.2.2 피처 추출
```python
async def extract_features(video_data: dict) -> dict:
    """비디오 데이터에서 피처 추출"""
    features = {}
    
    # 기본 피처
    features['view_count'] = video_data.get('view_count', 0)
    features['like_count'] = video_data.get('like_count', 0)
    features['comment_count'] = video_data.get('comment_count', 0)
    features['duration'] = video_data.get('duration', 0)
    
    # 파생 피처
    if features['view_count'] > 0:
        features['like_view_ratio'] = features['like_count'] / features['view_count']
        features['comment_view_ratio'] = features['comment_count'] / features['view_count']
        features['engagement_rate'] = (features['like_count'] + features['comment_count']) / features['view_count']
    else:
        features['like_view_ratio'] = 0
        features['comment_view_ratio'] = 0
        features['engagement_rate'] = 0
    
    # 시간 피처
    published_at = video_data.get('published_at')
    if published_at:
        dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
        features['hour'] = dt.hour
        features['day_of_week'] = dt.weekday()
        features['is_weekend'] = 1 if dt.weekday() >= 5 else 0
    
    # 카테고리 피처
    features['category_id'] = video_data.get('category_id', 0)
    
    return features
```

### 4.3 캐싱 전략

#### 4.3.1 다층 캐싱
```python
class MultiLayerCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.memory_cache = {}
        self.cache_ttl = {
            'prediction': 300,  # 5분
            'features': 600,    # 10분
            'search': 180       # 3분
        }
    
    async def get(self, key: str, cache_type: str = 'prediction'):
        """캐시에서 데이터 조회"""
        # 메모리 캐시 확인
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # Redis 캐시 확인
        cached_data = self.redis_client.get(key)
        if cached_data:
            data = json.loads(cached_data)
            # 메모리 캐시에 저장
            self.memory_cache[key] = data
            return data
        
        return None
    
    async def set(self, key: str, data: dict, cache_type: str = 'prediction'):
        """캐시에 데이터 저장"""
        ttl = self.cache_ttl.get(cache_type, 300)
        
        # Redis에 저장
        self.redis_client.setex(key, ttl, json.dumps(data))
        
        # 메모리 캐시에 저장
        self.memory_cache[key] = data
    
    async def invalidate(self, pattern: str):
        """패턴에 맞는 캐시 무효화"""
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)
        
        # 메모리 캐시에서도 제거
        for key in list(self.memory_cache.keys()):
            if pattern.replace('*', '') in key:
                del self.memory_cache[key]
```

## 5. Bloom Filter 통합

### 5.1 Bloom Filter 서비스

#### 5.1.1 Bloom Filter API
```python
class BloomFilterService:
    def __init__(self):
        self.bloom_filters = {}
        self.redis_client = redis.Redis(host='localhost', port=6379)
    
    async def check_video_exists(self, video_id: str, time_window_days: int = 7) -> bool:
        """비디오 존재 여부 확인"""
        # 시간 윈도우별 Bloom Filter 로드
        current_date = datetime.now().date()
        cutoff_date = current_date - timedelta(days=time_window_days)
        
        for i in range(time_window_days):
            check_date = cutoff_date + timedelta(days=i)
            filter_key = f"bloom_filter:{check_date}"
            
            # Redis에서 Bloom Filter 로드
            bloom_data = self.redis_client.get(filter_key)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                if bloom_filter.might_contain(video_id):
                    return True
        
        return False
    
    async def add_video_to_bloom(self, video_id: str, date: datetime = None):
        """Bloom Filter에 비디오 추가"""
        if date is None:
            date = datetime.now()
        
        date_key = date.date()
        filter_key = f"bloom_filter:{date_key}"
        
        # 기존 Bloom Filter 로드 또는 새로 생성
        bloom_data = self.redis_client.get(filter_key)
        if bloom_data:
            bloom_filter = pickle.loads(bloom_data)
        else:
            bloom_filter = DynamicBloomFilter(initial_capacity=100000, error_rate=0.01)
        
        # 비디오 ID 추가
        bloom_filter.add(video_id)
        
        # 업데이트된 Bloom Filter 저장
        self.redis_client.setex(filter_key, 86400, pickle.dumps(bloom_filter))  # 24시간 TTL
    
    async def get_bloom_filter_stats(self) -> dict:
        """Bloom Filter 통계"""
        stats = {
            "total_filters": 0,
            "total_capacity": 0,
            "estimated_items": 0,
            "false_positive_rate": 0.0
        }
        
        # 최근 7일 필터 통계
        current_date = datetime.now().date()
        for i in range(7):
            check_date = current_date - timedelta(days=i)
            filter_key = f"bloom_filter:{check_date}"
            
            bloom_data = self.redis_client.get(filter_key)
            if bloom_data:
                bloom_filter = pickle.loads(bloom_data)
                stats["total_filters"] += 1
                stats["total_capacity"] += bloom_filter.m
                stats["estimated_items"] += bloom_filter.count()
                stats["false_positive_rate"] += bloom_filter.error_rate
        
        if stats["total_filters"] > 0:
            stats["false_positive_rate"] /= stats["total_filters"]
        
        return stats
```

### 5.2 실시간 업데이트

#### 5.2.1 스트리밍 업데이트
```python
async def update_bloom_filter_stream():
    """스트리밍 데이터로 Bloom Filter 업데이트"""
    # 스트리밍 데이터 소스 연결
    streaming_df = spark.readStream \
        .format("delta") \
        .option("path", "data/silver/videos") \
        .load()
    
    def process_batch(batch_df, batch_id):
        """배치 처리"""
        bloom_service = BloomFilterService()
        
        for row in batch_df.collect():
            video_id = row['video_id']
            timestamp = row['event_timestamp']
            
            # Bloom Filter에 추가
            asyncio.run(bloom_service.add_video_to_bloom(video_id, timestamp))
    
    # 스트리밍 쿼리 시작
    query = streaming_df.writeStream \
        .foreachBatch(process_batch) \
        .option("checkpointLocation", "checkpoints/bloom_filter") \
        .start()
    
    return query
```

## 6. 성능 최적화

### 6.1 비동기 처리

#### 6.1.1 비동기 예측
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncPredictionService:
    def __init__(self, max_workers: int = 10):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.model_loader = ModelLoader()
        self.cache = MultiLayerCache()
    
    async def predict_batch(self, video_ids: List[str]) -> List[dict]:
        """배치 예측"""
        # 비동기 작업 생성
        tasks = [self._predict_single(video_id) for video_id in video_ids]
        
        # 병렬 실행
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 결과 처리
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "video_id": video_ids[i],
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _predict_single(self, video_id: str) -> dict:
        """단일 비디오 예측"""
        # 캐시 확인
        cache_key = f"prediction:{video_id}"
        cached_result = await self.cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # 데이터 조회 및 예측
        video_data = await get_video_data(video_id)
        features = await extract_features(video_data)
        prediction = await self.model_loader.predict("video_performance", features)
        
        # 결과 캐싱
        result = {
            "video_id": video_id,
            "prediction": prediction,
            "timestamp": datetime.now().isoformat()
        }
        
        await self.cache.set(cache_key, result)
        
        return result
```

### 6.2 데이터베이스 최적화

#### 6.2.1 연결 풀링
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseManager:
    def __init__(self, connection_string: str):
        self.engine = create_engine(
            connection_string,
            poolclass=QueuePool,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )
    
    async def get_video_data(self, video_id: str) -> dict:
        """비디오 데이터 조회"""
        query = """
        SELECT video_id, title, view_count, like_count, comment_count, 
               published_at, category_id, duration
        FROM videos 
        WHERE video_id = %s
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (video_id,))
            row = result.fetchone()
            
            if row:
                return dict(row._mapping)
            return None
    
    async def search_videos_by_keyword(self, keyword: str, limit: int = 10) -> List[dict]:
        """키워드 기반 비디오 검색"""
        query = """
        SELECT video_id, title, view_count, like_count, comment_count,
               published_at, category_id
        FROM videos 
        WHERE title ILIKE %s OR description ILIKE %s
        ORDER BY view_count DESC
        LIMIT %s
        """
        
        search_term = f"%{keyword}%"
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (search_term, search_term, limit))
            rows = result.fetchall()
            
            return [dict(row._mapping) for row in rows]
```

## 7. 모니터링 및 알림

### 7.1 성능 모니터링

#### 7.1.1 메트릭 수집
```python
import time
from prometheus_client import Counter, Histogram, Gauge, start_http_server

class MetricsCollector:
    def __init__(self):
        # 요청 메트릭
        self.request_count = Counter('api_requests_total', 'Total API requests', ['endpoint', 'method'])
        self.request_duration = Histogram('api_request_duration_seconds', 'API request duration', ['endpoint'])
        
        # 예측 메트릭
        self.prediction_count = Counter('predictions_total', 'Total predictions', ['model_name'])
        self.prediction_duration = Histogram('prediction_duration_seconds', 'Prediction duration', ['model_name'])
        
        # 시스템 메트릭
        self.active_connections = Gauge('active_connections', 'Active connections')
        self.cache_hit_ratio = Gauge('cache_hit_ratio', 'Cache hit ratio')
        
        # Prometheus 서버 시작
        start_http_server(8000)
    
    def record_request(self, endpoint: str, method: str, duration: float):
        """요청 메트릭 기록"""
        self.request_count.labels(endpoint=endpoint, method=method).inc()
        self.request_duration.labels(endpoint=endpoint).observe(duration)
    
    def record_prediction(self, model_name: str, duration: float):
        """예측 메트릭 기록"""
        self.prediction_count.labels(model_name=model_name).inc()
        self.prediction_duration.labels(model_name=model_name).observe(duration)
    
    def update_system_metrics(self, active_connections: int, cache_hit_ratio: float):
        """시스템 메트릭 업데이트"""
        self.active_connections.set(active_connections)
        self.cache_hit_ratio.set(cache_hit_ratio)

# 전역 메트릭 수집기
metrics_collector = MetricsCollector()
```

#### 7.1.2 알림 시스템
```python
class AlertManager:
    def __init__(self):
        self.alert_rules = {
            "high_response_time": {"threshold": 1.0, "severity": "warning"},
            "low_cache_hit_ratio": {"threshold": 0.7, "severity": "critical"},
            "high_error_rate": {"threshold": 0.05, "severity": "critical"}
        }
        self.alert_history = []
    
    async def check_alerts(self, metrics: dict):
        """알림 규칙 확인"""
        alerts = []
        
        # 응답 시간 알림
        if metrics.get('avg_response_time', 0) > self.alert_rules["high_response_time"]["threshold"]:
            alerts.append({
                "type": "high_response_time",
                "message": f"Average response time is {metrics['avg_response_time']:.2f}s",
                "severity": "warning",
                "timestamp": datetime.now()
            })
        
        # 캐시 히트율 알림
        if metrics.get('cache_hit_ratio', 1.0) < self.alert_rules["low_cache_hit_ratio"]["threshold"]:
            alerts.append({
                "type": "low_cache_hit_ratio",
                "message": f"Cache hit ratio is {metrics['cache_hit_ratio']:.2f}",
                "severity": "critical",
                "timestamp": datetime.now()
            })
        
        # 에러율 알림
        if metrics.get('error_rate', 0) > self.alert_rules["high_error_rate"]["threshold"]:
            alerts.append({
                "type": "high_error_rate",
                "message": f"Error rate is {metrics['error_rate']:.2f}",
                "severity": "critical",
                "timestamp": datetime.now()
            })
        
        # 알림 전송
        for alert in alerts:
            await self.send_alert(alert)
            self.alert_history.append(alert)
    
    async def send_alert(self, alert: dict):
        """알림 전송"""
        # 실제 구현에서는 Slack, 이메일, SMS 등으로 전송
        print(f"ALERT: {alert['severity'].upper()} - {alert['message']}")
```

---

**이전 문서**: [04_모델_학습_및_최적화_설계.md](./04_모델_학습_및_최적화_설계.md)  
**문서 완료**: 모든 설계 문서가 완성되었습니다.

