# 03. 피처 엔지니어링 및 라벨링 설계

## 1. 개요

### 1.1 목표
- 고급 피처 엔지니어링을 통한 예측 성능 향상
- HyperLogLog를 활용한 근사 유니크 카운팅
- CDF/PDF 기반 동적 라벨링 시스템 구축
- Gold Layer 피처 및 라벨 데이터 생성

### 1.2 핵심 요구사항
- **정확성**: 99% 이상 정확한 피처 계산
- **확장성**: 대용량 데이터 처리 가능
- **실시간성**: 1분 이내 피처 업데이트
- **해석가능성**: 피처의 의미와 중요도 명확화

## 2. 피처 엔지니어링 아키텍처

### 2.1 피처 파이프라인 구조

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Raw Data      │    │  Feature Eng.   │    │   Feature Store │
│                 │    │                 │    │                 │
│ • Bronze Layer  │───▶│ • HLL Counting  │───▶│ • Gold Layer    │
│ • Silver Layer  │    │ • CDF/PDF      │    │ • ML Features   │
│ • External APIs │    │ • Aggregations  │    │ • Labels        │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌─────────────────┐
                       │  Feature Store  │
                       │                 │
                       │ • Versioning    │
                       │ • Lineage      │
                       │ • Monitoring   │
                       └─────────────────┘
```

### 2.2 피처 카테고리

#### 2.2.1 기본 피처 (Basic Features)
- **수치형 피처**: 조회수, 좋아요 수, 댓글 수
- **비율형 피처**: 참여도, 조회 대비 좋아요 비율
- **시간형 피처**: 게시 시간, 업데이트 시간

#### 2.2.2 파생 피처 (Derived Features)
- **성장률 피처**: 조회수 증가율, 참여도 변화율
- **트렌드 피처**: 시간별 트렌드, 계절성 패턴
- **상호작용 피처**: 피처 간 상관관계

#### 2.2.3 집계 피처 (Aggregated Features)
- **채널별 집계**: 채널 평균 성과, 채널 트렌드
- **카테고리별 집계**: 카테고리 평균, 카테고리 순위
- **시간별 집계**: 시간대별 패턴, 요일별 패턴

## 3. HyperLogLog 설계

### 3.1 HLL 알고리즘 개요

#### 3.1.1 기본 원리
- **해시 함수**: 입력을 비트 시퀀스로 변환
- **Leading Zeros**: 해시 값의 앞자리 0 개수 계산
- **Register Array**: 각 버킷의 최대 leading zeros 저장
- **Cardinality Estimation**: 조화 평균을 통한 근사 계산

#### 3.1.2 알고리즘 구현
```python
import hashlib
import math

class HyperLogLog:
    def __init__(self, b: int = 4):
        self.b = b  # register 개수 = 2^b
        self.m = 2 ** b  # register 개수
        self.registers = [0] * self.m
        self.alpha = self._get_alpha(b)
        
    def _get_alpha(self, b: int) -> float:
        """정규화 상수 계산"""
        if b == 4:
            return 0.673
        elif b == 5:
            return 0.697
        elif b == 6:
            return 0.709
        else:
            return 0.7213 / (1 + 1.079 / self.m)
    
    def add(self, item: str):
        """아이템 추가"""
        # 해시 계산
        hash_value = int(hashlib.md5(item.encode()).hexdigest(), 16)
        
        # 버킷 인덱스 (하위 b 비트)
        bucket = hash_value & (self.m - 1)
        
        # leading zeros 계산 (상위 비트)
        value = hash_value >> self.b
        leading_zeros = self._count_leading_zeros(value)
        
        # register 업데이트
        self.registers[bucket] = max(self.registers[bucket], leading_zeros + 1)
    
    def _count_leading_zeros(self, value: int) -> int:
        """leading zeros 개수 계산"""
        if value == 0:
            return 32  # 32비트 기준
        return 32 - value.bit_length()
    
    def count(self) -> int:
        """근사 카디널리티 계산"""
        # 조화 평균 계산
        harmonic_mean = 0
        zero_registers = 0
        
        for register in self.registers:
            if register == 0:
                zero_registers += 1
            else:
                harmonic_mean += 1.0 / (2 ** register)
        
        # 조화 평균 기반 추정
        estimate = self.alpha * (self.m ** 2) / harmonic_mean
        
        # 작은 값 보정
        if estimate < 2.5 * self.m and zero_registers > 0:
            estimate = self.m * math.log(self.m / zero_registers)
        
        return int(estimate)
```

### 3.2 HLL 최적화

#### 3.2.1 Register 개수 최적화
```python
def optimal_register_count(expected_cardinality: int, error_rate: float = 0.01) -> int:
    """예상 카디널리티와 오차율에 따른 최적 register 개수"""
    # 오차율 = 1.04 / sqrt(m)
    m = int((1.04 / error_rate) ** 2)
    
    # 메모리 제한 고려 (최대 16MB)
    max_m = 16 * 1024 * 1024 // 4  # 4바이트 per register
    m = min(m, max_m)
    
    return m
```

#### 3.2.2 병합 연산
```python
def merge_hll(hll1: HyperLogLog, hll2: HyperLogLog) -> HyperLogLog:
    """두 HLL 병합"""
    if hll1.m != hll2.m:
        raise ValueError("Register 개수가 다름")
    
    merged_hll = HyperLogLog(hll1.b)
    for i in range(hll1.m):
        merged_hll.registers[i] = max(hll1.registers[i], hll2.registers[i])
    
    return merged_hll
```

### 3.3 Spark에서의 HLL 구현

#### 3.3.1 UDAF 구현
```python
from pyspark.sql.types import *
from pyspark.sql.functions import *

class HLLAggregator:
    def __init__(self, b: int = 4):
        self.b = b
        self.m = 2 ** b
        self.registers = [0] * self.m
        
    def update(self, value):
        if value is not None:
            hash_value = hash(str(value)) & 0xFFFFFFFF
            bucket = hash_value & (self.m - 1)
            value_bits = hash_value >> self.b
            leading_zeros = self._count_leading_zeros(value_bits)
            self.registers[bucket] = max(self.registers[bucket], leading_zeros + 1)
    
    def merge(self, other):
        for i in range(self.m):
            self.registers[i] = max(self.registers[i], other.registers[i])
    
    def evaluate(self):
        harmonic_mean = sum(1.0 / (2 ** r) for r in self.registers if r > 0)
        if harmonic_mean == 0:
            return 0
        alpha = 0.673 if self.b == 4 else 0.7213 / (1 + 1.079 / self.m)
        return int(alpha * (self.m ** 2) / harmonic_mean)
```

## 4. CDF/PDF 기반 라벨링

### 4.1 통계적 라벨링 개요

#### 4.1.1 CDF (Cumulative Distribution Function)
```python
def calculate_cdf(data: List[float]) -> Tuple[List[float], List[float]]:
    """CDF 계산"""
    sorted_data = sorted(data)
    n = len(sorted_data)
    x_values = sorted_data
    y_values = [(i + 1) / n for i in range(n)]
    return x_values, y_values
```

#### 4.1.2 PDF (Probability Density Function)
```python
def calculate_pdf(data: List[float], bins: int = 100) -> Tuple[List[float], List[float]]:
    """PDF 계산 (히스토그램 기반)"""
    min_val, max_val = min(data), max(data)
    bin_width = (max_val - min_val) / bins
    
    bin_counts = [0] * bins
    for value in data:
        bin_idx = min(int((value - min_val) / bin_width), bins - 1)
        bin_counts[bin_idx] += 1
    
    bin_centers = [min_val + (i + 0.5) * bin_width for i in range(bins)]
    probabilities = [count / len(data) for count in bin_counts]
    
    return bin_centers, probabilities
```

### 4.2 동적 임계값 설정

#### 4.2.1 백분위수 기반 임계값
```python
class PercentileThreshold:
    def __init__(self, percentile: float = 90.0):
        self.percentile = percentile
        
    def calculate_threshold(self, data: List[float]) -> float:
        """백분위수 기반 임계값 계산"""
        sorted_data = sorted(data)
        n = len(sorted_data)
        index = int((self.percentile / 100) * n)
        return sorted_data[min(index, n - 1)]
    
    def create_labels(self, data: List[float]) -> List[bool]:
        """임계값 기반 라벨 생성"""
        threshold = self.calculate_threshold(data)
        return [x >= threshold for x in data]
```

#### 4.2.2 적응형 임계값
```python
class AdaptiveThreshold:
    def __init__(self, window_size: int = 1000, sensitivity: float = 0.1):
        self.window_size = window_size
        self.sensitivity = sensitivity
        self.history = []
        
    def update_threshold(self, new_data: List[float]) -> float:
        """적응형 임계값 업데이트"""
        self.history.extend(new_data)
        if len(self.history) > self.window_size:
            self.history = self.history[-self.window_size:]
        
        # 이동 평균 기반 임계값
        if len(self.history) < 100:
            return np.percentile(self.history, 90)
        
        # 최근 데이터에 더 높은 가중치
        weights = np.exp(np.linspace(-1, 0, len(self.history)))
        weighted_percentile = np.average(sorted(self.history), weights=weights)
        
        return weighted_percentile * (1 + self.sensitivity)
```

### 4.3 다차원 라벨링

#### 4.3.1 다중 기준 라벨링
```python
class MultiCriteriaLabeler:
    def __init__(self, criteria: Dict[str, float]):
        self.criteria = criteria  # {"views": 0.4, "engagement": 0.3, "growth": 0.3}
        
    def calculate_composite_score(self, features: Dict[str, float]) -> float:
        """복합 점수 계산"""
        score = 0.0
        for criterion, weight in self.criteria.items():
            if criterion in features:
                # 정규화된 점수 (0-1)
                normalized_score = self._normalize_score(features[criterion], criterion)
                score += weight * normalized_score
        return score
    
    def _normalize_score(self, value: float, criterion: str) -> float:
        """기준별 정규화"""
        if criterion == "views":
            return min(value / 1000000, 1.0)  # 100만 조회수 기준
        elif criterion == "engagement":
            return min(value, 1.0)  # 0-1 범위
        elif criterion == "growth":
            return min(max(value, 0), 1.0)  # 0-1 범위
        return 0.0
```

#### 4.3.2 클러스터링 기반 라벨링
```python
from sklearn.cluster import KMeans

class ClusteringLabeler:
    def __init__(self, n_clusters: int = 3):
        self.n_clusters = n_clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        
    def fit_and_label(self, features: np.ndarray) -> np.ndarray:
        """클러스터링 기반 라벨 생성"""
        self.kmeans.fit(features)
        labels = self.kmeans.labels_
        
        # 클러스터별 평균 성과 계산
        cluster_means = []
        for i in range(self.n_clusters):
            cluster_mask = labels == i
            cluster_features = features[cluster_mask]
            cluster_mean = np.mean(cluster_features, axis=0)
            cluster_means.append(cluster_mean)
        
        # 성과 순으로 클러스터 정렬
        performance_scores = [np.mean(means) for means in cluster_means]
        sorted_indices = np.argsort(performance_scores)[::-1]
        
        # 성과 기반 라벨 재할당
        new_labels = np.zeros_like(labels)
        for i, cluster_idx in enumerate(sorted_indices):
            new_labels[labels == cluster_idx] = i
            
        return new_labels
```

## 5. 피처 스토어 설계

### 5.1 피처 스토어 아키텍처

#### 5.1.1 피처 스키마
```python
# 피처 스키마 정의
feature_schema = StructType([
    StructField("entity_id", StringType(), False),  # video_id
    StructField("feature_name", StringType(), False),
    StructField("feature_value", DoubleType(), True),
    StructField("feature_type", StringType(), False),  # numerical, categorical, text
    StructField("created_at", TimestampType(), False),
    StructField("valid_from", TimestampType(), False),
    StructField("valid_to", TimestampType(), True),
    StructField("feature_version", StringType(), False)
])
```

#### 5.1.2 피처 버전 관리
```python
class FeatureVersionManager:
    def __init__(self):
        self.versions = {}
        
    def create_version(self, feature_name: str, schema: StructType) -> str:
        """새 피처 버전 생성"""
        version_id = f"{feature_name}_v{len(self.versions.get(feature_name, [])) + 1}"
        if feature_name not in self.versions:
            self.versions[feature_name] = []
        self.versions[feature_name].append({
            "version_id": version_id,
            "schema": schema,
            "created_at": datetime.now()
        })
        return version_id
    
    def get_latest_version(self, feature_name: str) -> str:
        """최신 피처 버전 조회"""
        if feature_name not in self.versions or not self.versions[feature_name]:
            return None
        return self.versions[feature_name][-1]["version_id"]
```

### 5.2 피처 선형성 (Feature Lineage)

#### 5.2.1 피처 의존성 추적
```python
class FeatureLineage:
    def __init__(self):
        self.dependencies = {}  # feature -> [dependencies]
        self.lineage_graph = {}
        
    def add_dependency(self, feature: str, dependencies: List[str]):
        """피처 의존성 추가"""
        self.dependencies[feature] = dependencies
        self.lineage_graph[feature] = {
            "dependencies": dependencies,
            "created_at": datetime.now()
        }
    
    def get_feature_lineage(self, feature: str) -> Dict:
        """피처 선형성 조회"""
        if feature not in self.lineage_graph:
            return {}
        
        lineage = {
            "feature": feature,
            "direct_dependencies": self.lineage_graph[feature]["dependencies"],
            "all_dependencies": self._get_all_dependencies(feature),
            "created_at": self.lineage_graph[feature]["created_at"]
        }
        return lineage
    
    def _get_all_dependencies(self, feature: str) -> List[str]:
        """모든 의존성 조회 (재귀적)"""
        all_deps = set()
        if feature in self.dependencies:
            for dep in self.dependencies[feature]:
                all_deps.add(dep)
                all_deps.update(self._get_all_dependencies(dep))
        return list(all_deps)
```

### 5.3 피처 모니터링

#### 5.3.1 피처 품질 메트릭
```python
class FeatureQualityMonitor:
    def __init__(self):
        self.metrics = {}
        
    def calculate_quality_metrics(self, feature_data: DataFrame) -> Dict:
        """피처 품질 메트릭 계산"""
        metrics = {}
        
        # 기본 통계
        metrics["count"] = feature_data.count()
        metrics["null_count"] = feature_data.filter(col("feature_value").isNull()).count()
        metrics["null_rate"] = metrics["null_count"] / metrics["count"]
        
        # 수치형 피처 메트릭
        if feature_data.select("feature_value").dtypes[0][1] in ["double", "float", "int", "long"]:
            stats = feature_data.select("feature_value").describe().collect()
            metrics["mean"] = float(stats[1][1])
            metrics["std"] = float(stats[2][1])
            metrics["min"] = float(stats[3][1])
            metrics["max"] = float(stats[4][1])
            
            # 분포 메트릭
            metrics["skewness"] = self._calculate_skewness(feature_data)
            metrics["kurtosis"] = self._calculate_kurtosis(feature_data)
        
        return metrics
    
    def detect_drift(self, current_data: DataFrame, reference_data: DataFrame) -> Dict:
        """피처 드리프트 감지"""
        drift_metrics = {}
        
        # 통계적 드리프트
        current_mean = current_data.select(avg("feature_value")).collect()[0][0]
        reference_mean = reference_data.select(avg("feature_value")).collect()[0][0]
        drift_metrics["mean_drift"] = abs(current_mean - reference_mean) / reference_mean
        
        # 분포 드리프트 (Kolmogorov-Smirnov 테스트)
        drift_metrics["ks_statistic"] = self._calculate_ks_statistic(current_data, reference_data)
        
        return drift_metrics
```

## 6. 실시간 피처 계산

### 6.1 스트리밍 피처 계산

#### 6.1.1 실시간 집계 피처
```python
def create_streaming_features():
    """스트리밍 피처 정의"""
    return {
        # 시간 윈도우 기반 피처
        "views_1h": sum("view_count").over(
            Window.partitionBy("video_id")
                  .orderBy("event_timestamp")
                  .rangeBetween(-3600, 0)  # 1시간 윈도우
        ),
        
        "avg_engagement_1h": avg("engagement_score").over(
            Window.partitionBy("video_id")
                  .orderBy("event_timestamp")
                  .rangeBetween(-3600, 0)
        ),
        
        # 성장률 피처
        "view_growth_rate": (
            (col("current_views") - col("previous_views")) / col("previous_views")
        ),
        
        # 순위 피처
        "category_rank": rank().over(
            Window.partitionBy("category_id")
                  .orderBy(desc("view_count"))
        ),
        
        # 백분위수 피처
        "views_percentile": percent_rank().over(
            Window.partitionBy("category_id")
                  .orderBy("view_count")
        )
    }
```

#### 6.1.2 상태 기반 피처
```python
class StatefulFeatureCalculator:
    def __init__(self):
        self.state = {}
        
    def calculate_trend_feature(self, video_id: str, current_value: float, timestamp: datetime):
        """트렌드 피처 계산"""
        if video_id not in self.state:
            self.state[video_id] = {
                "values": [],
                "timestamps": [],
                "trend": 0.0
            }
        
        state = self.state[video_id]
        state["values"].append(current_value)
        state["timestamps"].append(timestamp)
        
        # 최근 10개 값만 유지
        if len(state["values"]) > 10:
            state["values"] = state["values"][-10:]
            state["timestamps"] = state["timestamps"][-10:]
        
        # 선형 회귀를 통한 트렌드 계산
        if len(state["values"]) >= 3:
            trend = self._calculate_linear_trend(state["values"])
            state["trend"] = trend
        
        return state["trend"]
    
    def _calculate_linear_trend(self, values: List[float]) -> float:
        """선형 트렌드 계산"""
        n = len(values)
        x = list(range(n))
        
        # 선형 회귀 계수 계산
        sum_x = sum(x)
        sum_y = sum(values)
        sum_xy = sum(x[i] * values[i] for i in range(n))
        sum_x2 = sum(x[i] ** 2 for i in range(n))
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        return slope
```

### 6.2 피처 캐싱

#### 6.2.1 Redis 기반 피처 캐시
```python
import redis
import json
import pickle

class FeatureCache:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.cache_ttl = 3600  # 1시간
        
    def get_feature(self, entity_id: str, feature_name: str):
        """피처 조회"""
        cache_key = f"feature:{entity_id}:{feature_name}"
        cached_value = self.redis_client.get(cache_key)
        
        if cached_value:
            return json.loads(cached_value)
        return None
    
    def set_feature(self, entity_id: str, feature_name: str, value, ttl: int = None):
        """피처 저장"""
        cache_key = f"feature:{entity_id}:{feature_name}"
        ttl = ttl or self.cache_ttl
        
        self.redis_client.setex(cache_key, ttl, json.dumps(value))
    
    def invalidate_feature(self, entity_id: str, feature_name: str):
        """피처 무효화"""
        cache_key = f"feature:{entity_id}:{feature_name}"
        self.redis_client.delete(cache_key)
```

## 7. 피처 선택 및 최적화

### 7.1 피처 중요도 분석

#### 7.1.1 상관관계 분석
```python
def analyze_feature_correlation(features_df: DataFrame, target_column: str) -> DataFrame:
    """피처 상관관계 분석"""
    # 수치형 컬럼만 선택
    numeric_columns = [col for col, dtype in features_df.dtypes if dtype in ["double", "float", "int", "long"]]
    
    # 상관관계 매트릭스 계산
    correlation_matrix = features_df.select(numeric_columns).corr()
    
    # 타겟과의 상관관계만 추출
    target_correlations = correlation_matrix.filter(col("feature") == target_column)
    
    return target_correlations.orderBy(desc("correlation"))
```

#### 7.1.2 피처 중요도 점수
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression

class FeatureImportanceAnalyzer:
    def __init__(self):
        self.importance_scores = {}
        
    def calculate_importance(self, X, y, method: str = "random_forest"):
        """피처 중요도 계산"""
        if method == "random_forest":
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X, y)
            self.importance_scores = dict(zip(X.columns, rf.feature_importances_))
            
        elif method == "f_score":
            selector = SelectKBest(score_func=f_regression, k="all")
            selector.fit(X, y)
            self.importance_scores = dict(zip(X.columns, selector.scores_))
        
        return self.importance_scores
    
    def select_top_features(self, k: int = 10) -> List[str]:
        """상위 k개 피처 선택"""
        sorted_features = sorted(self.importance_scores.items(), 
                               key=lambda x: x[1], reverse=True)
        return [feature for feature, score in sorted_features[:k]]
```

### 7.2 피처 엔지니어링 자동화

#### 7.2.1 자동 피처 생성
```python
class AutomatedFeatureEngineering:
    def __init__(self):
        self.feature_generators = []
        
    def add_feature_generator(self, generator_func):
        """피처 생성기 추가"""
        self.feature_generators.append(generator_func)
    
    def generate_features(self, df: DataFrame) -> DataFrame:
        """자동 피처 생성"""
        result_df = df
        
        for generator in self.feature_generators:
            result_df = generator(result_df)
        
        return result_df

# 피처 생성기 예시
def create_ratio_features(df: DataFrame) -> DataFrame:
    """비율 피처 생성"""
    return df.withColumn("like_view_ratio", col("like_count") / col("view_count")) \
            .withColumn("comment_view_ratio", col("comment_count") / col("view_count")) \
            .withColumn("engagement_score", 
                       (col("like_count") + col("comment_count")) / col("view_count"))

def create_time_features(df: DataFrame) -> DataFrame:
    """시간 피처 생성"""
    return df.withColumn("hour", hour("published_at")) \
            .withColumn("day_of_week", dayofweek("published_at")) \
            .withColumn("is_weekend", when(col("day_of_week").isin([1, 7]), 1).otherwise(0))
```

---

**이전 문서**: [02_스트리밍_데이터_처리_설계.md](./02_스트리밍_데이터_처리_설계.md)  
**다음 문서**: [04_모델_학습_및_최적화_설계.md](./04_모델_학습_및_최적화_설계.md)

